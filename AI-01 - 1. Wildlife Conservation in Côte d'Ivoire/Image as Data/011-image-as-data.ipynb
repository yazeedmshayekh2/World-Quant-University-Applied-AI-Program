{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">‚úó</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">‚úó</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start this lesson there are two things we need to do. First, we need to import the libraries that we'll need to get our work done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, it's a good idea to print out the version numbers for our libraries, including Python. That way, anyone who reviews our work we'll know exactly what software we used in case they want to reproduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"matplotlib version:\", matplotlib.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"PIL version:\", PIL.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In future lessons, we'll add a few more \"getting ready\" steps. For now, we're ready to start. üèéÔ∏èüí®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Tensors in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.1:** Use the nested list `my_values` to create the tensor `my_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_values = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "my_tensor = ...\n",
    "\n",
    "print(\"my_tensor class:\", type(my_tensor))\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.2:** Print the dimensions and data type of `my_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"my_tensor shape:\",...) \n",
    "\n",
    "print(\"my_tensor dtype:\", ...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors also have a `.device` attribute, which specifies the hardware on which it's stored. By default, tensors are created on the computer's CPU. Let's check if that's the case for  `my_tensor`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.3:** Print the device of `my_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"my_tensor device:\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some computers come with GPUs, which allow for bigger and faster model building. In PyTorch, the `cuda` package is used to access GPUs on Linux and Windows machines; `mps` is used on Macs. Let's check what's available on the WQU virtual machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPUs available via `cuda`\n",
    "cuda_gpus_available = torch.cuda.is_available()\n",
    "\n",
    "# Check if GPUs available via `mps`\n",
    "mps_gpus_available = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"cuda GPUs available:\", cuda_gpus_available)\n",
    "print(\"mps GPUs available:\", mps_gpus_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have access to GPUs! To take advantage of this, we can change the tensor's device by using the `.to()` method. But note that if you are pushing tensors to a device, you have to reassign them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.4:** Change the device of `my_tensor` to `\"cuda\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "\"raises-exception\""
    ]
   },
   "outputs": [],
   "source": [
    "my_tensor = ...\n",
    "\n",
    "print(\"my_tensor device:\", my_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be able to see a performance boost with this tensor because it's already very small. However, switching devices will definitely speed up data preprocessing and models in later lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.5:** Slice `my_tensor`, assigning its top two rows to `left_tensor` and its bottom two rows to `right_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_tensor = ...\n",
    "right_tensor = ...\n",
    "\n",
    "print(\"left_tensor class:\", type(left_tensor))\n",
    "print(\"left_tensor shape:\", left_tensor.shape)\n",
    "print(\"left_tensor data type:\", left_tensor.dtype)\n",
    "print(\"left_tensor device:\", left_tensor.device)\n",
    "print(left_tensor)\n",
    "print()\n",
    "print(\"right_tensor class:\", type(right_tensor))\n",
    "print(\"right_tensor shape:\", right_tensor.shape)\n",
    "print(\"right_tensor data type:\", right_tensor.dtype)\n",
    "print(\"right_tensor device:\", right_tensor.device)\n",
    "print(right_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.6:** Use both the mathematical operator and the class method to add `left_tensor` to `right_tensor`. Assign the results to `summed_tensor_operator` and `summed_tensor_method`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_tensor_operator = ...\n",
    "summed_tensor_method = ...\n",
    "\n",
    "print(\"summed_tensor_operator class:\", type(summed_tensor_operator))\n",
    "print(\"summed_tensor_operator shape:\", summed_tensor_operator.shape)\n",
    "print(\"summed_tensor_operator data type:\", summed_tensor_operator.dtype)\n",
    "print(\"summed_tensor_operator device:\", summed_tensor_operator.device)\n",
    "print(summed_tensor_operator)\n",
    "print()\n",
    "print(\"summed_tensor_method class:\", type(summed_tensor_method))\n",
    "print(\"summed_tensor_method shape:\", summed_tensor_method.shape)\n",
    "print(\"summed_tensor_method data type:\", summed_tensor_method.dtype)\n",
    "print(\"summed_tensor_method device:\", summed_tensor_method.device)\n",
    "print(summed_tensor_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important mathematical operations in deep learning is multiplication, so let's spend some time on it here.\n",
    "\n",
    "Keep in mind that, when it comes to tensors, there's more than one type of multiplication. For starters, there's **element-wise multiplication**, where the corresponding values of two tensors are multiplied together. In PyTorch, we can do this using the `*` operator or the `.mul()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.7:** Use both the mathematical operator and the class method to multiply `left_tensor` to `right_tensor`. Assign the results to `ew_tensor_operator` and `ew_tensor_method`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_tensor_operator = ...\n",
    "ew_tensor_method = ...\n",
    "\n",
    "print(\"ew_tensor_operator class:\", type(ew_tensor_operator))\n",
    "print(\"ew_tensor_operator shape:\", ew_tensor_operator.shape)\n",
    "print(\"ew_tensor_operator data type:\", ew_tensor_operator.dtype)\n",
    "print(\"ew_tensor_operator device:\", ew_tensor_operator.device)\n",
    "print(ew_tensor_operator)\n",
    "print()\n",
    "print(\"ew_tensor_method class:\", type(ew_tensor_method))\n",
    "print(\"ew_tensor_method shape:\", ew_tensor_method.shape)\n",
    "print(\"ew_tensor_method data type:\", ew_tensor_method.dtype)\n",
    "print(\"ew_tensor_method device:\", ew_tensor_method.device)\n",
    "print(ew_tensor_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that element-wise multiplication is **commutative**. It doesn't matter in what order we multiply the two tensors. The product of `left_tensor * right_tensor` is the same as the product of `right_tensor * left_tensor`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_tensor * right_tensor == right_tensor * left_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, there's **matrix multiplication**, which combines the rows and columns of two tensors to generate a new one. We can use the `@` operator or the `.matmul()` method.\n",
    "\n",
    "To see how this works, let's create two new tensors with different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_left_tensor = torch.Tensor([[2, 5], [7, 3]])\n",
    "new_right_tensor = torch.Tensor([[8], [9]])\n",
    "\n",
    "print(\"new_left_tensor class:\", type(new_left_tensor))\n",
    "print(\"new_left_tensor shape:\", new_left_tensor.shape)\n",
    "print(\"new_left_tensor data type:\", new_left_tensor.dtype)\n",
    "print(\"new_left_tensor device:\", new_left_tensor.device)\n",
    "print(new_left_tensor)\n",
    "print()\n",
    "print(\"new_right_tensor class:\", type(new_right_tensor))\n",
    "print(\"new_right_tensor shape:\", new_right_tensor.shape)\n",
    "print(\"new_right_tensor data type:\", new_right_tensor.dtype)\n",
    "print(\"new_right_tensor device:\", new_right_tensor.device)\n",
    "print(new_right_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's multiply them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 1.1.8:** Use both the mathematical operator and the class method to perform matrix multiplication on `new_left_tensor` and `new_right_tensor`. Assign the results to `mm_tensor_operator` and `mm_tensor_method`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_tensor_operator = ...\n",
    "mm_tensor_method = ...\n",
    "\n",
    "print(\"mm_tensor_operator class:\", type(mm_tensor_operator))\n",
    "print(\"mm_tensor_operator shape:\", mm_tensor_operator.shape)\n",
    "print(\"mm_tensor_operator data type:\", mm_tensor_operator.dtype)\n",
    "print(\"mm_tensor_operator device:\", mm_tensor_operator.device)\n",
    "print(mm_tensor_operator)\n",
    "print()\n",
    "print(\"mm_tensor_method class:\", type(mm_tensor_method))\n",
    "print(\"mm_tensor_method shape:\", mm_tensor_method.shape)\n",
    "print(\"mm_tensor_method data type:\", mm_tensor_method.dtype)\n",
    "print(\"mm_tensor_method device:\", mm_tensor_method.device)\n",
    "print(mm_tensor_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important thing to remember: matrix multiplication is **not commutative**. The number of columns in the tensor on the left must equal the number of rows in the tensor on the right. If these two dimensions don't match, your code will throw a `RunTimeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "mm_tensor_operator = new_right_tensor @ new_left_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Matrix multiplication is the way your models will train and make predictions, and dimension mismatches will be a common source of bugs when you start building models. For that reason, it's always important to check the shape of your tensors. ü§ì\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, tensors come with methods for aggregation calculations. For instance, if we wanted to know the mean of all the elements in `my_tensor`, we'd use the `.mean()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.9:** Calculate the mean for all values in `my_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor_mean = ...\n",
    "\n",
    "print(\"my_tensor_mean class:\", type(my_tensor_mean))\n",
    "print(\"my_tensor_mean shape:\", my_tensor_mean.shape)\n",
    "print(\"my_tensor_mean data type:\", my_tensor_mean.dtype)\n",
    "print(\"my_tensor_mean device:\", my_tensor_mean.device)\n",
    "print(\"my_tensor mean:\", my_tensor_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.mean()` by itself is helpful if we want to aggregate all the elements in a tensor, but more often we want to aggregate along one of the tensor's axes. \n",
    "\n",
    "For example, what's the mean of each column in `my_tensor`? Remember that the dimensions of this tensor: `[4, 3]`. The first number in this list refers to the 4 rows, and the second to 3 columns. If we want the mean of each column, we need to reduce across the rows or first dimension. To do this, we use the `dim=` argument. And since Python uses 0-based indexing, we specify the first dimension with a `0`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.10:** Calculate the mean for each column in `my_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor_column_means = ...\n",
    "\n",
    "print(\"my_tensor_column_means class:\", type(my_tensor_column_means))\n",
    "print(\"my_tensor_column_means shape:\", my_tensor_column_means.shape)\n",
    "print(\"my_tensor_column_means data type:\", my_tensor_column_means.dtype)\n",
    "print(\"my_tensor_column_means device:\", my_tensor_column_means.device)\n",
    "print(\"my_tensor column means:\", my_tensor_column_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have some helpful tools for working with PyTorch tensors. We can check a tensor's shape, data type and device. We can manipulate a tensor by slicing it. We can perform mathematical operations on tensors, including matrix multiplication and aggregation calculations. \n",
    "\n",
    "Up next, let's apply our new skills by exploring the dataset for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.11:** Following the pattern of `data_dir`, assign the path to the multi-class training data to `train_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"data_p1\", \"data_multiclass\")\n",
    "train_dir = ...\n",
    "\n",
    "print(\"data_dir class:\", type(data_dir))\n",
    "print(\"Data directory:\", data_dir)\n",
    "print()\n",
    "print(\"train_dir class:\", type(train_dir))\n",
    "print(\"Training data directory:\", train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll list the contents of our training directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.12:** Create a list of the contents of `train_dir`, and assign the result to `class_directories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_directories = ...\n",
    "\n",
    "print(\"class_directories type:\", type(class_directories))\n",
    "print(\"class_directories length:\", len(class_directories))\n",
    "print(class_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our training directory contains 8 subdirectories. Judging by their names, each contains the images for one of the classes in our dataset.\n",
    "\n",
    "Now that we know how our data is organized, let's check the distribution of our classes. In order to do this we'll need to count the number of files in each subdirectory. We'll store our results in a [pandas](https://pandas.pydata.org/pandas-docs/version/2.2/index.html) `Series()` for easy data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.13:** Complete the `for` loop so that `class_distributions_dict` contains the name of each subdirectory as its keys and the number of files in each subdirectory as its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distributions_dict = {}\n",
    "\n",
    "for subdirectory in class_directories:\n",
    "   ...\n",
    "\n",
    "class_distributions = pd.Series(class_distributions_dict)\n",
    "\n",
    "print(\"class_distributions type:\", type(class_distributions))\n",
    "print(\"class_distributions shape:\", class_distributions.shape)\n",
    "print(class_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bar chart from `class_distributions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.14:** Create a bar chart from `class_distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of class distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot the data\n",
    "ax....  # Write your code here\n",
    "ax.set_xlabel(\"Class Label\")\n",
    "ax.set_ylabel(\"Frequency [count]\")\n",
    "ax.set_title(\"Class Distribution, Multiclass Training Set\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for hog image\n",
    "hog_image_path = os.path.join(train_dir, \"hog\", \"ZJ000072.jpg\")\n",
    "\n",
    "# Define path for antelope image\n",
    "antelope_image_path = os.path.join(train_dir, \"antelope_duiker\", \"ZJ002533.jpg\")\n",
    "\n",
    "print(\"hog_image_path type:\", type(hog_image_path))\n",
    "print(hog_image_path)\n",
    "print()\n",
    "print(\"antelope_image_path type:\", type(antelope_image_path))\n",
    "print(antelope_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load these images, we'll use the [Pillow](https://pillow.readthedocs.io/en/stable/index.html) library (aka PIL), which comes with lots of tools for image processing. We'll start with the hog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_image_pil = Image.open(hog_image_path)\n",
    "\n",
    "print(\"hog_image_pil type:\", type(hog_image_pil))\n",
    "hog_image_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, the antelope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.15:** Use PIL to open `antelope_image_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antelope_image_pil = ...\n",
    "\n",
    "print(\"antelope_image_pil type:\", type(antelope_image_pil))\n",
    "antelope_image_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any differences in the way these images look? Let's keep using PIL to explore further, looking at their `.size` and `.mode` attributes. Again, we'll start with the hog and then do the antelope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image size\n",
    "hog_image_pil_size = hog_image_pil.size\n",
    "\n",
    "# Get image mode\n",
    "hog_image_pil_mode = hog_image_pil.mode\n",
    "\n",
    "# Print results\n",
    "print(\"hog_image_pil_size class:\", type(hog_image_pil_size))\n",
    "print(\"hog_image_pil_size length:\", len(hog_image_pil_size))\n",
    "print(\"Hog image size:\", hog_image_pil_size)\n",
    "print()\n",
    "print(\"hog_image_pil_mode class:\", type(hog_image_pil_mode))\n",
    "print(\"Hog image mode:\", hog_image_pil_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.16:** Get the `.size` and `.mode` attributes from `antelope_image_pil` and assign the results to `antelope_image_pil_size` and `antelope_image_pil_mode`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image size\n",
    "antelope_image_pil_size = ...\n",
    "\n",
    "# Get image mode\n",
    "antelope_image_pil_mode = ...\n",
    "\n",
    "# Get image mode\n",
    "print(\"antelope_image_pil_size class:\", type(antelope_image_pil_size))\n",
    "print(\"antelope_image_pil_size length:\", len(antelope_image_pil_size))\n",
    "print(\"Antelope image size:\", antelope_image_pil_size)\n",
    "print()\n",
    "print(\"antelope_image_pil_mode class:\", type(antelope_image_pil_mode))\n",
    "print(\"Antelope image mode:\", antelope_image_pil_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these attributes, we can confirm that there are two differences between our images.\n",
    "\n",
    "- **Mode:** The hog image is in grayscale (`mode=\"L\"`), while the antelope image is in color mode (`mode=\"RGB\"`).\n",
    "- **Size:** The hog images is smaller than the antelope image.\n",
    "\n",
    "These differences are important because all the images in our dataset must have the same size and mode before we can use them to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_tensor = transforms.ToTensor()(hog_image_pil)\n",
    "\n",
    "print(\"hog_tensor type:\", type(hog_tensor))\n",
    "print(\"hog_tensor shape:\", hog_tensor.shape)\n",
    "print(\"hog_tensor dtype:\", hog_tensor.dtype)\n",
    "print(\"hog_tensor device:\", hog_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Take a moment to examine the syntax we used to convert the hog image into a tensor.üîç <code>ToTensor()</code> is a class. (You can check out the class definition <a href=\"https://pytorch.org/vision/main/_modules/torchvision/transforms/transforms.html#ToTensor\">here</a>.) However, we're using it like a function, combining it with another set of parenthesis that contains <code>hog_image_pill</code> as if it was an argument.\n",
    "\n",
    "\n",
    "The reason this works is that the <code>ToTensor()</code> class definition includes a <code>\\_\\_call\\_\\_</code> method. This allows us to use the class like a function. Keep this in mind for the next lesson, where we'll create our own class for transforming images. ü§ì\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing to `antelope_image_pil`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.17:** Convert `antelope_image_pil` to a tensor and assign the result to `antelope_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antelope_tensor = ...\n",
    "\n",
    "print(\"antelope_tensor type:\", type(antelope_tensor))\n",
    "print(\"antelope_tensor shape:\", antelope_tensor.shape)\n",
    "print(\"antelope_tensor dtype:\", antelope_tensor.dtype)\n",
    "print(\"antelope_tensor device:\", antelope_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of these two tensors, we can see that they're both 3-dimensional. We can also see that some of the dimensions correspond to image height and width. For example, the shape of `hog_tensor` is `[1, 360, 640]`. The image's height is 360 pixels, and it's width is 640 pixels. But what does the first dimension correspond to? What does the `1` mean?\n",
    "\n",
    "In addition to height and width, image files generally come with **color channels**. A color channel holds information about the intensity of a specific color for each pixel in an image. Because our hog image is grayscale, there's only one color to represent: gray. In fact, if we extract the values from the gray channel in `hog_tensor` and plot them, we end up with the same image we saw in the last section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with single axis\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Plot gray channel of hog_tensor\n",
    "ax.imshow(hog_tensor[0, :, :])\n",
    "\n",
    "# Turn off x- and y-axis\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Hog, grayscale\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the hog image is grayscale, the antelope image is in color. Its mode is RGB, which stands red, green, and blue. Each of these colors has its own channel in the image. That's where the `3` in the `antelope_tensor` shape `[3, 540, 960]` comes from. We can extract the values for each channel using our slicing skills and plot them side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.18:** Complete the code below to plot the red, green, and blue channels of `antelope_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 subplots\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot red channel\n",
    "red_channel = antelope_tensor[0, :, :]\n",
    "ax0.imshow(red_channel, cmap=\"Reds\")\n",
    "ax0.set_title(\"Antelope, Red Channel\")\n",
    "ax0.axis(\"off\")\n",
    "\n",
    "# Plot green channel\n",
    "green_channel = ...\n",
    "\n",
    "\n",
    "\n",
    "# Plot blue channel\n",
    "blue_channel = ...\n",
    "\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key takeaway is that the dimensions for an image tensor are always **(C x H x W)**, channel by height by width.\n",
    "\n",
    "We know how the values in an image tensor are organized, but we haven't looked at the values themselves. Focusing on the `antelope_tensor` only, let's check its minimum and maximum values using the `.amax()` and `.amin()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.19:** Calculate the minimum and maximum values of `antelope_tensor` and assign the results to `max_channel_values` and `min_channel_values`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_channel_values = ...\n",
    "min_channel_values = ...\n",
    "\n",
    "print(\"max_channel_values class:\", type(max_channel_values))\n",
    "print(\"max_channel_values shape:\", max_channel_values.shape)\n",
    "print(\"max_channel_values data type:\", max_channel_values.dtype)\n",
    "print(\"max_channel_values device:\", max_channel_values.device)\n",
    "print(\"Max values in antelope_tensor:\", max_channel_values)\n",
    "print()\n",
    "print(\"min_channel_values class:\", type(min_channel_values))\n",
    "print(\"min_channel_values shape:\", min_channel_values.shape)\n",
    "print(\"min_channel_values data type:\", min_channel_values.dtype)\n",
    "print(\"min_channel_values device:\", min_channel_values.device)\n",
    "print(\"Min values in antelope_tensor:\", min_channel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the values in the tensor range from 0 to 1. 0 means that the color intensity at a particular pixel is 0%; 1 means intensity is 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "It's equally common to see the values in an image tensor range from 0 to 255. In fact, that's how the values in our image files are actually stored. However, the <code>ToTensor()</code> class <a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html#totensor\">automatically converts</a> PIL images from <code>[0, 255]</code> to <code>[0, 1]</code>. So it's always a good idea to double-check image tensor values before building a model. ü§ì\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this lesson, we'll do an aggregation calculation to find the mean value for each color channel in `antelope_tensor`. Remember that the color channel is the first dimension in the tensor (index position `0` in Python). This means we want to reduce along the other two dimensions, height and width. They are at index positions `1` and `2`, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1.20:** Calculate the mean values of the separate color channels in `antelope_tensor` and assign the result to `mean_channel_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_channel_values = ...\n",
    "\n",
    "print(\"mean_channel_values class:\", type(mean_channel_values))\n",
    "print(\"mean_channel_values shape:\", mean_channel_values.shape)\n",
    "print(\"mean_channel_values dtype:\", mean_channel_values.dtype)\n",
    "print(\"mean_channel_values device:\", mean_channel_values.device)\n",
    "print(\"Mean channel values in antelope_tensor (RGB):\", mean_channel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! We'll see why it's important to calculate the mean of each color channel in the following lesson. For now, here are the key discoveries we've made about our dataset in this lesson:\n",
    "\n",
    "- Our dataset is organized into folders. We have data for a binary classification model and a multi-class model. In both cases, the training data is divided into subdirectories, one for each class.\n",
    "- The images in our dataset come in different sizes.\n",
    "- The images in our dataset come in different modes (grayscale and RGB).\n",
    "- When we convert our images from PIL to tensors, their values range from `0` to `1`.\n",
    "\n",
    "In the next lesson, we'll build tools to combine our images into a uniform dataset of tensors. We'll also build and train a binary classification model using PyTorch. See you there soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
